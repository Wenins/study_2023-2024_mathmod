---
## Front matter
title: "Научно-исследовательская работа"
subtitle: "Named Entity Recognition (NER)"
author: "Артамонов Тимофей Евгеньевич"

## Generic otions
lang: ru-RU
toc-title: "Содержание"

## Bibliography
bibliography: bib/cite.bib
csl: pandoc/csl/gost-r-7-0-5-2008-numeric.csl

## Pdf output format
toc: true # Table of contents
toc-depth: 2
lof: true # List of figures
lot: true # List of tables
fontsize: 12pt
linestretch: 1.5
papersize: a4
documentclass: scrreprt
## I18n polyglossia
polyglossia-lang:
  name: russian
  options:
	- spelling=modern
	- babelshorthands=true
polyglossia-otherlangs:
  name: english
## I18n babel
babel-lang: russian
babel-otherlangs: english
## Fonts
mainfont: PT Serif
romanfont: PT Serif
sansfont: PT Sans
monofont: PT Mono
mainfontoptions: Ligatures=TeX
romanfontoptions: Ligatures=TeX
sansfontoptions: Ligatures=TeX,Scale=MatchLowercase
monofontoptions: Scale=MatchLowercase,Scale=0.9
## Biblatex
biblatex: true
biblio-style: "gost-numeric"
biblatexoptions:
  - parentracker=true
  - backend=biber
  - hyperref=auto
  - language=auto
  - autolang=other*
  - citestyle=gost-numeric
## Pandoc-crossref LaTeX customization
figureTitle: "Рис."
tableTitle: "Таблица"
listingTitle: "Листинг"
lofTitle: "Список иллюстраций"
lotTitle: "Список таблиц"
lolTitle: "Листинги"
## Misc options
indent: true
header-includes:
  - \usepackage{indentfirst}
  - \usepackage{float} # keep figures where there are in the text
  - \floatplacement{figure}{H} # keep figures where there are in the text
---

# Введение

## Цель работы

Научиться использовать модели для NER, обучать их, дообучать их и подготавливать для них данные.

## Задачи

- Исследовать задачу NER
- Рассмотреть методы преобработки данных
- Рассмотреть методы обучения собственной модели
- Рассмотреть методы дообучения модели с использованием библиотеки transformers
- Дообучить свою модель
- Проанализировать результаты, сравнив её с готовой моделью

## Введение

Text analysis is quite a labor-intensive task, especially nowadays when it can involve entire databases with hundreds of thousands of lines of text. 
Extracting information from such text can take a considerable amount of time and effort from individuals involved in the task. 
Yet, the task of extracting information from text and analyzing it is becoming increasingly important for decision-making in various fields, including business and equally important areas like medicine.
To tackle such tasks, Natural Language Processing (NLP) technologies are employed. One of the key tasks in NLP is Named Entity Recognition (NER). 
Named Entity Recognition is the task of identifying and classifying named entities from text. The main entities include people’s names, geographical and political locations, and organizations. This approach allows for the automatic extraction of important information from unstructured text, which can then be used for various purposes, such as database creation or automation of various text-related processes.
Neural networks are the primary tool for solving the named entity recognition task. These networks are pre-trained on large volumes of data and are capable of accurately predicting the correct entity tag.
The goal of this work is to learn how to utilize such models, train and fine-tune pre-trained models, and also explore text vectorization methods.

# Основная часть

## Предобработка данных

В любой задаче NLP, как следует из названия, мы имеем дело с текстом. Посмотрим на самый очевидный и распространенный пример нейросети - Chat-GPT. Эта нейросеть, в качестве входных данных, принимает текстовое сообщение от пользователя и, в качестве выходных данных, выдает ответ так же в виде текстового сообщения. Может показаться, что нейросеть понимает наш язык и даже может его использовать. На самом деле, все нейросети, независимо от области их работы: вычисления, текст, изображения, все они работают только с числами.
Как же так получается? Данные, с которыми работает нейросеть, в первую очередь, мы поговорим об обучении, подвергаются обезательной предобработке. В нашем случае, мы работаем с текстом. Здесь все не так просто, как с изображениями. Если в случае с изображениями мы просто можем представить каждый пиксель, как массив, определяющий его цвет, то в случае с текстом не получится просто каждому слову присвоить индекс, ведь наша речь не так проста. Нельзя рассматривать слова отдельно друг от друга - в виде индексов, ведь каждое предложение представляет собой грамматически организованное соединение слов, обладающее смысловой и интонационной законченностью. Допустим, мы можем в случае нашей задачи NER - опустить интонацию, т.к. нам не важно какой тональности предложение, но смысловую состовляющую, которая формируется как раз из того, что все слова в предложении связаны с другим, мы не можем. Решение этой задачи, конечно, уже нашли. Для этого, нам необходимо сначала перевести все слова в индексы, часто их индексируют в зависимости от частоты их использования в определенном датасете. После, эти индексы переводятся в многомерные вектора, которые и реализуют смысловую состовляющую. В итоге, из списка слов, мы уже получаем список векторов действительных чисел, с которыми наша нейросеть уже сможет работать. Так же необходимо сделать так, чтобы все предложения были одинаковой длины, т.к. модель будет работать с тензорами. Для этого используются функции truncation(), которая обрезает все предложения, если они длиннее определенного лимита, и функция padding(), которая увеличивает размерность предложения до определенной, если ее длина ниже, добавляя нейтральные элементы.
Но на этом предобработка текста не заканчивается. По сути, задача NER - это задача многоклассовой классификации, где мы должны каждому слову присвоить свой класс. Конечный список классов, зависит от конкретной задачи, но самые общие классы или сущности - PER(Person), ORG(Organization), LOC(Location), O(not entity). Быстро скажем о том, как обучается модель решать задачу многоклассовой классификации. Мы имеем набор, в котором каждое значение разбито на 2 части: первая часть - текстовое предложение(tokens), вторая часть - список, определяющий класс, каждого слова в предложении(labels).  Из-за особенностей функций, используемых в нейронных сетях, необходимо также и преобразовать значения меток(labels) - тех самых значений, определяющих класс слова. Обычно, в датасетах столбец labels представлен в виде списка с метками класса в формате строк, либо в формате индексов, означающих этот класс. В любом случае, нам нужно сделать так, чтобы каждому слову соответствовал список из нулей и единицы по индексу, определяющему этот класс. Такой способ преобразования получил название one-hot encoding.
После того, как мы перевели наши текстовые данные в численные вектора и применили one-hot encoding для наших labels, мы наконец готовы приступить к обучению модели.

## Обучение модели с нуля

Если мы хотим обучить сво свобственную модель с нуля, то нам нужно самим определиться с её архитектурой. Для каждой задачи, нужно применять определенную архитектуру и определенный подход. Некоторые слои лучше работают с решением одной задачи и хуже с решением другой, но это не значит, что всё уже давно определено. Обучение нейронной сети это все ещё творческий процесс, для каждой сети с ее набором данных нужно найти свой подход, эксперементируя с различными коэффициентами, слоями, функциями потерь и прочее.
Но все же, так или иначе, нейросети одинаковой сложности для одной и той же задачи будут выглядеть похоже. 
Перейдем к практической части. Учитывая, что мы работаем с текстовыми данными, нам нужно обрабатывать всё предложение в целом, а не отдельно каждое слово. Так как мы обрабатываем каждое слово в списке последовательно, а традиционные нейросети не обладают свойством памяти, то есть каждый шаг, они забывают предыдущий. Из-за этого будет проблематично учесть контекст предложения, даже несмотря на векторизацию. С этим может помочь RNN или Recurrent Neural Network - это сети, которые имеют обратную связь и позволяют сохранять информацию, это своего рода память. Мы будем использовать Long Short-Term Memory или LSTM - модификации сети RNN. Сети на основе LSTM добились успехов во многих задачах, таких как классификация, распознавание изображений и т.д., превосходя свои свои стандартные версии. Если быть точнее, то мы будем использовать BiLSTM или Bidirectional LSTM, чтобы находить bidirectional long-term dependecies. Последним слоем обязательно должен быть, слой с количество нейронов, равным количеству классов и функцией активации softmax, это многопеременная логистическая функция - это обобщение логистической функции для многомерного случая. Функция преобразует итоговый вектор в вектор той же размерности, где каждая координата полученного вектора представлена вещественным числом в интервале [0,1] и сумма координат равна 1. Для задачи многоклассовой классификации так же наиболее эффективно использовать функцию потерь categorical cross-entropy. Остальное остается на усмотрение разработчика, часто для предотвращения переобучения используют слой Dropout, который откючает определенный процент нейронов каждую эпоху. Так же для предовращения переобучения, ведь это самая большая проблема в машинном обучении, лучше использовать такой инструмент как callback, который при последовательном снижении показателей модели при обучении автоматически остановит обучение модели. Всё же остальное, а именно: количество эпох, начальный коэффициент обучения, оптимизатор и размер батча - это то, с чем надо экспериментировать и подбирать самостоятельно. После этого модель можно скомпилировать и обучить на своих данных.

## Использование библиотеки transormers

Все перечисленное уже реализовано в таких крупных библиотеках как tensorflow и pytorch, но есть также библиотека transformers, которая появилась не так давно. Используя её можно использовать уже pretrained модели, которые до этого обучилась на большом количестве данных и дообучить её на конкретной задаче. Благодаря более широким знаниям, чем строго её область применения, модель будет устойчива к выбросам и будет способна дать верный ответ, даже получив необычные входные данные. Можно даже использовать fine-tuned модели, от известных компаний, которые выложены в открытом доступе и имеют отличные показатели.

### Использование pretrained модели
Также такую модель уже можно использовать для вычисления. Это легко сделать с помощью инструмента pipeline(). Для этого необходимо предварительно имортировать pipeline из библиотеки transformers. В качестве параметра этой функции необходимо указать задачу, которую необходимо решить, присвоив это переменной. После этого можно давать одно или же несколько предложений, но списком этой переменной и, например, если мы решаем задачу классификации текста, то есть определения тональности, то на выходе мы получим POSITIVE или NEGATIVE и уверенность модели в её ответе.

### Fine-tuning of pretrained model

Самые популярные и используемые модели для работы с текстом это BERT и GPT, так как мы занимаемся задачей многоклассовой классификации, мы будем использовать bert-base model, так как с такими задачами BERT справляется лучше. Выбор конкретной bert-base-cased модели зависит от конкретной задачи и пользователя. To fine-tune pretrained model нужно гораздо меньше усилий, если сравнивать это с обучением собственной модели. Необходимо так же preprocess данные, можно использовать библиотеку AutoTokenizer() from transformers, в параметры токенизатора можно передать параметры для padding и truncation. После необходимо загрузить саму модель из библиотеки transformers, она загрузится на устройство и сохранится в кэше. Дальше необходимо скомпилировать модель использую метод compile(), где мы можем задать оптимизатор и начальный коэффициент обучения, фукнцию активации указывать не нужно. После этого можно перейти к непосредственному обучения модели используя метод fit(), передав в него либо список из токенов и меток, либо датасет с соответствующим разделением, если мы в пункте preprocess сделали разделенение на тренировочную и валидационную выборку, то можно также в качестве валидационных данных указать наши массивы или датасет. Если же мы этого не сделали, то мы все равно можем использовать валидацию, задав коэффициент, который перейдет из тренировочной выборки в валидационную на время обучения. Использование валидации необходимо, для отслеживания процесса обучения, в том числе для отслеживания переобучения. Так же можно сохранить все промежуточные данные из процесса обучения, чтобы после построить графики, отображающие прогрессию модели. 

### Использование уже обученной модели

Так же используя pipeline() можно использовать готовую обученную модель для нужной вам задачи. Самый простой метод - воспользоваться сайтом Hugging Face, где вы можете найти модель на выбранном языке, которая будет решать выбранную задачу. Необходимо указать в параметрах функции pipeline() название модели, которую вы хотите использовать, задачу, которую эта модель будет решать. Либо можно отдельно импортировать токенизатор этой модели используя метод from_pretrained(), так же указав в параметрах название модели, после, используя тот же метод нужно задать саму модель.

# Выводы

В работе были исследована одна из задач Natural Langueage Proccessing, а именно Named Entity Recognition, которая в наше время является одной из самых актуальных. Также были рассмотрены методы решения этой задачи, используя различные инструменты, в частности предварительная обработка данных, обучение модели, дообучение модели, использование уже готовых моделей 

# Список литературы{.unnumbered}

::: {#refs}
:::
